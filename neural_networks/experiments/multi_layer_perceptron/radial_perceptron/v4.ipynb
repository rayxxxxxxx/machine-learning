{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dsy7TBhOwLTP"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.functional import normalize\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"using: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HV4FmEgwlJr",
        "outputId": "7defc09c-8b8b-4c89-e5d5-e1efb7bfe007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(n, i):\n",
        "    q = np.zeros(n)\n",
        "    q[i] = 1.0\n",
        "    return q\n",
        "\n",
        "\n",
        "class MNISTDataset(Dataset):\n",
        "    def __init__(self, fp):\n",
        "\n",
        "        xy = np.loadtxt(fp, delimiter=',', dtype=np.float32)\n",
        "\n",
        "        self.x = torch.from_numpy(xy[:, 1:])\n",
        "        self.x /= 255.0\n",
        "\n",
        "        labels = []\n",
        "        for i in xy[:, 0]:\n",
        "            labels.append(one_hot(10, int(i)))\n",
        "\n",
        "        self.y = torch.from_numpy(np.array(labels))\n",
        "        self.n_samples = xy.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples"
      ],
      "metadata": {
        "id": "FOqp_fNSYPbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MNISTDataset(Path('/content/sample_data/mnist_train_small.csv'))\n",
        "test_dataset = MNISTDataset(Path('/content/sample_data/mnist_test.csv'))"
      ],
      "metadata": {
        "id": "LidSecGmZTnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size)"
      ],
      "metadata": {
        "id": "N9imMsimdq7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron(nn.Module):\n",
        "    def __init__(self, n_layers, n_hidden, n_in, n_out):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_in = n_in\n",
        "        self.n_out = n_out\n",
        "\n",
        "        self.hidden = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(self.n_in, self.n_hidden),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(self.n_hidden, self.n_in)\n",
        "            ) for i in range(self.n_layers)\n",
        "        ])\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.n_in, self.n_out),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = [l(x) for l in self.hidden]\n",
        "        h = torch.stack(h, 0)\n",
        "        h = torch.sum(h, 0)\n",
        "        # h = x + h\n",
        "\n",
        "        y = self.classifier(h)\n",
        "\n",
        "        return y"
      ],
      "metadata": {
        "id": "gwAQCNdC8Ig_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    for (x, y) in dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_pred = model(x)\n",
        "\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "h1WEdfCC-B7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    nbatches = len(dataloader)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    loss = 0\n",
        "    true_positive = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (x, y) in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            y_pred = model(x)\n",
        "\n",
        "            loss += loss_fn(y_pred, y).item()\n",
        "            true_positive += (y_pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "\n",
        "    return (loss / nbatches, true_positive / size)"
      ],
      "metadata": {
        "id": "SvUOBg3G_Pzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epoch = 16\n",
        "learning_rate = 1e-2"
      ],
      "metadata": {
        "id": "i6JITgOno3i_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_layers = 4\n",
        "n_hidden = 8\n",
        "n_in = 784\n",
        "n_out = 10\n",
        "\n",
        "model = Perceptron(n_layers, n_hidden, n_in, n_out).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "-_PEq2iBo2ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_weights = n_in * n_hidden * 2 * n_layers + n_in * n_out\n",
        "num_biases = n_hidden * n_layers + n_in * n_layers + n_out\n",
        "model_size = num_weights + num_biases\n",
        "\n",
        "print(f'model size: {model_size}')\n",
        "print(f'total weights: {num_weights}')\n",
        "print(f'total biases: {num_biases}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYtbkzcXkRmo",
        "outputId": "0296306c-6b51-49d5-a488-edf3afc62a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model size: 61194\n",
            "total weights: 58016\n",
            "total biases: 3178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(n_epoch):\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    loss, acc = test(test_dataloader, model, loss_fn)\n",
        "    print(f\"Epoch: {i + 1}; loss: {round(loss, 3)}; accuracy: {round(acc * 100, 2)}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvkZGut5BBvz",
        "outputId": "93329871-be08-42fc-ffdd-9250ead41d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1; loss: 1.563; accuracy: 90.73%\n",
            "Epoch: 2; loss: 1.543; accuracy: 92.25%\n",
            "Epoch: 3; loss: 1.538; accuracy: 92.59%\n",
            "Epoch: 4; loss: 1.528; accuracy: 93.57%\n",
            "Epoch: 5; loss: 1.528; accuracy: 93.44%\n",
            "Epoch: 6; loss: 1.524; accuracy: 93.81%\n",
            "Epoch: 7; loss: 1.523; accuracy: 93.86%\n",
            "Epoch: 8; loss: 1.521; accuracy: 94.0%\n",
            "Epoch: 9; loss: 1.519; accuracy: 94.17%\n",
            "Epoch: 10; loss: 1.517; accuracy: 94.46%\n",
            "Epoch: 11; loss: 1.514; accuracy: 94.78%\n",
            "Epoch: 12; loss: 1.516; accuracy: 94.58%\n",
            "Epoch: 13; loss: 1.511; accuracy: 94.99%\n",
            "Epoch: 14; loss: 1.513; accuracy: 94.83%\n",
            "Epoch: 15; loss: 1.513; accuracy: 94.88%\n",
            "Epoch: 16; loss: 1.512; accuracy: 94.86%\n"
          ]
        }
      ]
    }
  ]
}