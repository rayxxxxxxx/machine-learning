{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path('..', '..', 'data', 'iris_csv.csv'))\n",
    "\n",
    "for c in df.columns[0:4]:\n",
    "    df[c] = (df[c]-df[c].mean())/df[c].std()\n",
    "\n",
    "df['synth1'] = df['petallength']*df['petalwidth']\n",
    "df['synth2'] = df['sepallength']*df['petallength']\n",
    "df['synth3'] = df['sepallength']*df['petalwidth']\n",
    "\n",
    "for name in df['class'].unique():\n",
    "    df[f'label-{name}'] = df['class'].map(lambda x: 1 if x == name else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_frac = 0.8\n",
    "\n",
    "# np.random.seed(0)\n",
    "# p = np.random.permutation(df.index.size)\n",
    "\n",
    "# test_size = int(p.size*test_frac)\n",
    "# train_size = int(p.size*(1-test_frac))\n",
    "\n",
    "# idx_test = p[0 : test_size]\n",
    "# idx_train = p[test_size: p.size]\n",
    "\n",
    "# features_columns = ['sepallength', 'sepalwidth', 'petallength', 'petalwidth']\n",
    "# label_columns = ['Iris-setosa_label', 'Iris-versicolor_label', 'Iris-virginica_label']\n",
    "\n",
    "# xTest = np.array(df.iloc[idx_test][features_columns])\n",
    "# yTest = np.array(df.iloc[idx_test][label_columns])\n",
    "\n",
    "# xTrain = np.array(df.iloc[idx_train][features_columns])\n",
    "# yTrain = np.array(df.iloc[idx_train][label_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "setosa_idxs = np.arange(0, 50)\n",
    "versicolor_idxs = np.arange(50, 100)\n",
    "virginica_idxs = np.arange(100, 150)\n",
    "\n",
    "p = np.random.permutation(np.arange(50))\n",
    "\n",
    "setosa_train_idxs = setosa_idxs[p[0:10]]\n",
    "setosa_test_idxs = setosa_idxs[p[10:]]\n",
    "\n",
    "versicolor_train_idxs = versicolor_idxs[p[0:10]]\n",
    "versicolor_test_idxs = versicolor_idxs[p[10:]]\n",
    "\n",
    "virginica_train_idxs = virginica_idxs[p[0:10]]\n",
    "virginica_test_idxs = virginica_idxs[p[10:]]\n",
    "\n",
    "feature_columns = ['sepallength', 'sepalwidth', 'petallength', 'petalwidth']\n",
    "label_columns = ['label-Iris-setosa', 'label-Iris-versicolor', 'label-Iris-virginica']\n",
    "\n",
    "xTrain = np.vstack([\n",
    "    df.iloc[setosa_train_idxs][feature_columns],\n",
    "    df.iloc[versicolor_train_idxs][feature_columns],\n",
    "    df.iloc[virginica_train_idxs][feature_columns]\n",
    "])\n",
    "\n",
    "yTrain = np.vstack([\n",
    "    df.iloc[setosa_train_idxs][label_columns],\n",
    "    df.iloc[versicolor_train_idxs][label_columns],\n",
    "    df.iloc[virginica_train_idxs][label_columns]\n",
    "])\n",
    "\n",
    "xTest = np.vstack([\n",
    "    df.iloc[setosa_test_idxs][feature_columns],\n",
    "    df.iloc[versicolor_test_idxs][feature_columns],\n",
    "    df.iloc[virginica_test_idxs][feature_columns]\n",
    "])\n",
    "\n",
    "yTest = np.vstack([\n",
    "    df.iloc[setosa_test_idxs][label_columns],\n",
    "    df.iloc[versicolor_test_idxs][label_columns],\n",
    "    df.iloc[virginica_test_idxs][label_columns]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(x: np.ndarray) -> float:\n",
    "    # return np.clip(x,-1, 1)\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def F_jit(x: np.ndarray) -> np.ndarray:\n",
    "    # return np.clip(x,-1, 1)\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def dF_jit(x: np.ndarray) -> np.ndarray:\n",
    "    # return np.array([0 if xi <= -1 or xi >= 1 else 1 for xi in x])\n",
    "    return 1-np.square(np.tanh(x))\n",
    "\n",
    "\n",
    "def Softmax(x: np.ndarray) -> float:\n",
    "    y = np.exp(x)\n",
    "    return y/np.sum(y)\n",
    "\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def Softmax_jit(x: np.ndarray) -> float:\n",
    "    y = np.exp(x)\n",
    "    return y/np.sum(y)\n",
    "\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def grads(xBatch: np.ndarray, yBatch: np.ndarray, Wy: np.ndarray, Wh: np.ndarray, Bh:np.ndarray, Bx:np.ndarray, b:np.ndarray) -> tuple[np.ndarray]:\n",
    "    dWh = np.zeros(Wh.shape)\n",
    "    dWy = np.zeros(Wy.shape)\n",
    "    \n",
    "    dBh = np.zeros(Bh.shape)\n",
    "    dBx = np.zeros(Bx.shape)\n",
    "    db = np.zeros(b.shape)\n",
    "\n",
    "    Uh = np.zeros((Wh.shape[0], Wh.shape[2]))\n",
    "    Yh = np.zeros((Wh.shape[0], Wh.shape[1]))\n",
    "    \n",
    "    for i in prange(xBatch.shape[0]):\n",
    "        Uh *= 0\n",
    "        Yh *= 0\n",
    "\n",
    "        for j in prange(Wh.shape[0]):\n",
    "            Uh[j] = xBatch[i] @ Wh[j] + Bh[j]\n",
    "            Yh[j] = F_jit(Uh[j]) @ Wh[j].T + Bx[j]\n",
    "\n",
    "        Xh = np.sum(Yh, axis=0)\n",
    "        \n",
    "        u = Xh @ Wy + b\n",
    "        y = Softmax_jit(u)\n",
    "                \n",
    "        for j in prange(Wh.shape[0]):\n",
    "            dWh[j] += (y-yBatch[i]) @ Wy.T @ Wh[j] * dF_jit(Uh[j]) * np.atleast_2d(xBatch[i]).T\n",
    "            dBh[j] += (y-yBatch[i]) @ Wy.T @ Wh[j] * dF_jit(Uh[j])\n",
    "            dBx[j] += (y-yBatch[i]) @ Wy.T\n",
    "        \n",
    "        dWy += (y-yBatch[i]) * np.atleast_2d(Xh).T\n",
    "        db += y-yBatch[i]\n",
    "    \n",
    "    return (dWh, dWy, dBh, dBx, db)\n",
    "\n",
    "class RBMPerceptron:\n",
    "    def __init__(self, nL:int, nH:int, nIn:int, nOut:int) -> None:\n",
    "        self.nL = nL\n",
    "        self.nH = nH\n",
    "        self.nIn = nIn\n",
    "        self.nOut = nOut\n",
    "                \n",
    "        self.Wh: np.ndarray = np.random.uniform(-1, 1, (nL, nIn, nH))\n",
    "        self.Wy: np.ndarray = np.random.uniform(-1, 1, (nIn, nOut))\n",
    "        \n",
    "        self.Bh: np.ndarray = np.zeros((nL, nH))\n",
    "        self.Bx: np.ndarray = np.zeros((nL, nIn))\n",
    "        self.b: np.ndarray = np.zeros(nOut)\n",
    "\n",
    "    def predict(self, x:np.ndarray) -> np.ndarray:\n",
    "        Yh = np.zeros(self.nIn)\n",
    "        \n",
    "        for i in range(self.Wh.shape[0]):\n",
    "            Yh += F(x @ self.Wh[i] + self.Bh[i]) @ self.Wh[i].T + self.Bx[i]\n",
    "            \n",
    "        return Softmax(Yh @ self.Wy + self.b)\n",
    "\n",
    "    def train(self, xTrain: np.ndarray, yTrain: np.ndarray, lr, batch_size, max_iter) -> None:\n",
    "        n = xTrain.shape[0]\n",
    "\n",
    "        for k in range(max_iter):\n",
    "            idxs = np.random.choice(a=np.arange(n), size=batch_size, replace=False)\n",
    "            \n",
    "            dWh, dWy, dBh, dBx, db = grads(xTrain[idxs], yTrain[idxs], self.Wy, self.Wh, self.Bh, self.Bx, self.b)\n",
    "            \n",
    "            self.Wh -= lr*dWh\n",
    "            self.Wy -= lr*dWy\n",
    "            \n",
    "            self.Bh -= lr*dBh\n",
    "            self.Bx -= lr*dBx\n",
    "            self.b -= lr*db\n",
    "        \n",
    "    def loss(self, x: np.ndarray, y: np.ndarray) -> float:\n",
    "        Ypred = np.array([self.predict(xi) for xi in x])\n",
    "        h = -1/self.nOut * np.sum(y*np.log(Ypred), axis=1)\n",
    "        return 1/x.shape[0] * np.sum(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "untrained loss: 1.532463\n",
      "trained loss: 0.094877\n",
      "0.9416666666666667\n"
     ]
    }
   ],
   "source": [
    "nL = 4\n",
    "nH = 4\n",
    "nIn = 4\n",
    "nOut = 3\n",
    "\n",
    "lr = 1e-2\n",
    "batch_size = 30\n",
    "max_iter = 1000\n",
    "\n",
    "model = RBMPerceptron(nL, nH, nIn, nOut)\n",
    "\n",
    "print('untrained loss: {0:.6f}'.format(model.loss(xTest, yTest)))\n",
    "\n",
    "model.train(xTrain, yTrain, lr, batch_size, max_iter)\n",
    "\n",
    "print('trained loss: {0:.6f}'.format(model.loss(xTest, yTest)))\n",
    "\n",
    "yPred = np.array([model.predict(x) for x in xTest])\n",
    "\n",
    "c = 0\n",
    "for y, yp in zip(yTest, yPred):\n",
    "    c += 1 if np.argmax(y) == np.argmax(yp) else 0\n",
    "    \n",
    "accuracy = c / xTest.shape[0]\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 [1 0 0] [1. 0. 0.]\n",
      "  1 [1 0 0] [1. 0. 0.]\n",
      "  2 [1 0 0] [1. 0. 0.]\n",
      "  3 [1 0 0] [1. 0. 0.]\n",
      "  4 [1 0 0] [1. 0. 0.]\n",
      "  5 [1 0 0] [1. 0. 0.]\n",
      "  6 [1 0 0] [1. 0. 0.]\n",
      "  7 [1 0 0] [1. 0. 0.]\n",
      "  8 [1 0 0] [1. 0. 0.]\n",
      "  9 [1 0 0] [1. 0. 0.]\n",
      " 10 [1 0 0] [1. 0. 0.]\n",
      " 11 [1 0 0] [1. 0. 0.]\n",
      " 12 [1 0 0] [1. 0. 0.]\n",
      " 13 [1 0 0] [1. 0. 0.]\n",
      " 14 [1 0 0] [1. 0. 0.]\n",
      " 15 [1 0 0] [1. 0. 0.]\n",
      " 16 [1 0 0] [1. 0. 0.]\n",
      " 17 [1 0 0] [1. 0. 0.]\n",
      " 18 [1 0 0] [1. 0. 0.]\n",
      " 19 [1 0 0] [1. 0. 0.]\n",
      " 20 [1 0 0] [1. 0. 0.]\n",
      " 21 [1 0 0] [1. 0. 0.]\n",
      " 22 [1 0 0] [1. 0. 0.]\n",
      " 23 [1 0 0] [1. 0. 0.]\n",
      " 24 [1 0 0] [1. 0. 0.]\n",
      " 25 [1 0 0] [1. 0. 0.]\n",
      " 26 [1 0 0] [1. 0. 0.]\n",
      " 27 [1 0 0] [1. 0. 0.]\n",
      " 28 [1 0 0] [1. 0. 0.]\n",
      " 29 [1 0 0] [1. 0. 0.]\n",
      " 30 [1 0 0] [1. 0. 0.]\n",
      " 31 [1 0 0] [1. 0. 0.]\n",
      " 32 [1 0 0] [1. 0. 0.]\n",
      " 33 [1 0 0] [1. 0. 0.]\n",
      " 34 [1 0 0] [1. 0. 0.]\n",
      " 35 [1 0 0] [1. 0. 0.]\n",
      " 36 [1 0 0] [1. 0. 0.]\n",
      " 37 [1 0 0] [1. 0. 0.]\n",
      " 38 [1 0 0] [1. 0. 0.]\n",
      " 39 [1 0 0] [1. 0. 0.]\n",
      " 40 [0 1 0] [0.   0.32 0.68]\n",
      " 41 [0 1 0] [0.   0.92 0.08]\n",
      " 42 [0 1 0] [0. 1. 0.]\n",
      " 43 [0 1 0] [0.   0.11 0.89]\n",
      " 44 [0 1 0] [0. 1. 0.]\n",
      " 45 [0 1 0] [0.03 0.97 0.  ]\n",
      " 46 [0 1 0] [0.05 0.95 0.  ]\n",
      " 47 [0 1 0] [0. 1. 0.]\n",
      " 48 [0 1 0] [0.26 0.74 0.  ]\n",
      " 49 [0 1 0] [0. 1. 0.]\n",
      " 50 [0 1 0] [0. 1. 0.]\n",
      " 51 [0 1 0] [0. 1. 0.]\n",
      " 52 [0 1 0] [0. 1. 0.]\n",
      " 53 [0 1 0] [0.   0.67 0.33]\n",
      " 54 [0 1 0] [0. 1. 0.]\n",
      " 55 [0 1 0] [0. 0. 1.]\n",
      " 56 [0 1 0] [0. 1. 0.]\n",
      " 57 [0 1 0] [0. 1. 0.]\n",
      " 58 [0 1 0] [0. 1. 0.]\n",
      " 59 [0 1 0] [0. 1. 0.]\n",
      " 60 [0 1 0] [0. 1. 0.]\n",
      " 61 [0 1 0] [0. 1. 0.]\n",
      " 62 [0 1 0] [0. 1. 0.]\n",
      " 63 [0 1 0] [0. 1. 0.]\n",
      " 64 [0 1 0] [0. 1. 0.]\n",
      " 65 [0 1 0] [0. 1. 0.]\n",
      " 66 [0 1 0] [0. 1. 0.]\n",
      " 67 [0 1 0] [0. 1. 0.]\n",
      " 68 [0 1 0] [0.   0.99 0.01]\n",
      " 69 [0 1 0] [0. 1. 0.]\n",
      " 70 [0 1 0] [0. 1. 0.]\n",
      " 71 [0 1 0] [0. 1. 0.]\n",
      " 72 [0 1 0] [0. 1. 0.]\n",
      " 73 [0 1 0] [0.01 0.99 0.  ]\n",
      " 74 [0 1 0] [0. 1. 0.]\n",
      " 75 [0 1 0] [0. 1. 0.]\n",
      " 76 [0 1 0] [0. 1. 0.]\n",
      " 77 [0 1 0] [0. 1. 0.]\n",
      " 78 [0 1 0] [0. 1. 0.]\n",
      " 79 [0 1 0] [0. 1. 0.]\n",
      " 80 [0 0 1] [0. 1. 0.]\n",
      " 81 [0 0 1] [0. 0. 1.]\n",
      " 82 [0 0 1] [0.   0.05 0.95]\n",
      " 83 [0 0 1] [0.   0.97 0.03]\n",
      " 84 [0 0 1] [0. 0. 1.]\n",
      " 85 [0 0 1] [0.   0.07 0.93]\n",
      " 86 [0 0 1] [0. 0. 1.]\n",
      " 87 [0 0 1] [0. 0. 1.]\n",
      " 88 [0 0 1] [0. 0. 1.]\n",
      " 89 [0 0 1] [0. 1. 0.]\n",
      " 90 [0 0 1] [0. 0. 1.]\n",
      " 91 [0 0 1] [0.   0.14 0.86]\n",
      " 92 [0 0 1] [0. 0. 1.]\n",
      " 93 [0 0 1] [0. 0. 1.]\n",
      " 94 [0 0 1] [0. 0. 1.]\n",
      " 95 [0 0 1] [0. 0. 1.]\n",
      " 96 [0 0 1] [0. 0. 1.]\n",
      " 97 [0 0 1] [0.   0.13 0.87]\n",
      " 98 [0 0 1] [0. 0. 1.]\n",
      " 99 [0 0 1] [0.   0.04 0.96]\n",
      "100 [0 0 1] [0. 0. 1.]\n",
      "101 [0 0 1] [0. 0. 1.]\n",
      "102 [0 0 1] [0. 0. 1.]\n",
      "103 [0 0 1] [0. 0. 1.]\n",
      "104 [0 0 1] [0. 0. 1.]\n",
      "105 [0 0 1] [0. 0. 1.]\n",
      "106 [0 0 1] [0. 0. 1.]\n",
      "107 [0 0 1] [0. 0. 1.]\n",
      "108 [0 0 1] [0. 0. 1.]\n",
      "109 [0 0 1] [0.   0.15 0.85]\n",
      "110 [0 0 1] [0. 0. 1.]\n",
      "111 [0 0 1] [0. 0. 1.]\n",
      "112 [0 0 1] [0.   0.99 0.01]\n",
      "113 [0 0 1] [0. 0. 1.]\n",
      "114 [0 0 1] [0. 0. 1.]\n",
      "115 [0 0 1] [0.   0.01 0.99]\n",
      "116 [0 0 1] [0. 0. 1.]\n",
      "117 [0 0 1] [0. 0. 1.]\n",
      "118 [0 0 1] [0. 0. 1.]\n",
      "119 [0 0 1] [0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for x,y in zip(xTest, yTest):\n",
    "    print(\"{0:-3} {1} {2}\".format(i, y, model.predict(x).round(2)))\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
