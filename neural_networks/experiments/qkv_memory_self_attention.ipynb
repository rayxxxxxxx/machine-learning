{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOuFBbijCnPi"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "abc = ' ' + string.punctuation + string.digits + string.ascii_letters\n",
        "len_abc = len(abc)\n",
        "print(f'{abc=}\\n{len_abc=}')\n",
        "\n",
        "itoc = {i:c for i,c in enumerate(abc)}\n",
        "ctoi = {c:i for i,c in enumerate(abc)}\n",
        "\n",
        "def encode(s: str) -> list[int]:\n",
        "    return [ctoi[c] for c in s]\n",
        "\n",
        "def decode(l: list[int]) -> str:\n",
        "    return ''.join([itoc[i] for i in l])\n",
        "\n",
        "def enc2tnsr(l: list[int]) -> torch.Tensor:\n",
        "    return torch.tensor(l).long()\n",
        "\n",
        "def enc2seq(l: list[int]) -> torch.Tensor:\n",
        "    return F.one_hot(enc2tnsr(l), len_abc).float()\n",
        "\n",
        "def enct2seq(t: torch.Tensor) -> torch.Tensor:\n",
        "    return F.one_hot(t, len_abc).float()\n",
        "\n",
        "def str2seq(s: str) -> torch.Tensor:\n",
        "    encoded = torch.tensor(encode(s)).long()\n",
        "    return F.one_hot(encoded, len_abc).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, in_dim, key_dim) -> None:\n",
        "        super(AttentionHead, self).__init__()\n",
        "\n",
        "        self.attn_scale = key_dim**-0.5\n",
        "\n",
        "        self.q = nn.Linear(in_dim, key_dim, bias=False)\n",
        "        self.k = nn.Linear(in_dim, key_dim, bias=False)\n",
        "        self.v = nn.Linear(in_dim, in_dim, bias=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        Q, K, V = self.q(x), self.k(x), self.v(x)\n",
        "        A = (Q @ K.transpose(1,2)) * self.attn_scale\n",
        "        A = F.softmax(A, dim=2)\n",
        "        return A @ V\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, key_dim, in_dim) -> None:\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([AttentionHead(in_dim, key_dim//n_heads) for _ in range(n_heads)])\n",
        "        self.proj = nn.Linear(n_heads*in_dim, in_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        t = torch.cat([h(x) for h in self.heads], dim=2)\n",
        "        return self.proj(t)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, in_dim, key_dim, n_heads, h_dim) -> None:\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.attention = MultiHeadAttention(n_heads, key_dim, in_dim)\n",
        "        self.mlp = nn.Sequential(nn.Linear(in_dim, h_dim), nn.ReLU(), nn.Linear(h_dim, in_dim))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x + self.attention(x)\n",
        "        return x + self.mlp(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMmVk4WcxNKD"
      },
      "outputs": [],
      "source": [
        "class WriteHead(nn.Module):\n",
        "    def __init__(self, in_dim, n_mem, mem_dim, key_dim) -> None:\n",
        "        super(WriteHead, self).__init__()\n",
        "\n",
        "        self.scaler = key_dim**-0.5\n",
        "\n",
        "        self.q = nn.Linear(in_dim, key_dim, bias=False)\n",
        "        self.k = nn.Linear(mem_dim, key_dim, bias=False)\n",
        "        self.v = nn.Linear(in_dim, mem_dim, bias=False)\n",
        "        self.g = nn.Parameter(torch.rand(mem_dim, n_mem))\n",
        "\n",
        "    def forward(self, x: torch.Tensor, M: torch.Tensor) -> torch.Tensor:\n",
        "        Q, K, V = self.q(x), self.k(M), self.v(x)\n",
        "        A = F.softmax(K @ Q.transpose(-1,-2) * self.scaler, dim=-2)\n",
        "        G = F.sigmoid(V @ self.g @ M)\n",
        "        return A @ V * G + (1-A) * M * (1-G)\n",
        "\n",
        "\n",
        "class ReadHead(nn.Module):\n",
        "    def __init__(self, in_dim, n_mem, mem_dim, key_dim) -> None:\n",
        "        super(ReadHead, self).__init__()\n",
        "\n",
        "        self.scaler = key_dim**-0.5\n",
        "\n",
        "        self.q = nn.Linear(in_dim, key_dim, bias=False)\n",
        "        self.k = nn.Linear(mem_dim, key_dim, bias=False)\n",
        "        self.v = nn.Linear(mem_dim, in_dim, bias=False)\n",
        "        self.g = nn.Parameter(torch.rand(in_dim, n_mem))\n",
        "\n",
        "    def forward(self, x: torch.Tensor, M: torch.Tensor) -> torch.Tensor:\n",
        "        Q, K, V = self.q(x), self.k(M), self.v(M)\n",
        "        A = F.softmax(Q @ K.transpose(-1,-2) * self.scaler, dim=-1)\n",
        "        G = F.sigmoid(x @ self.g @ V)\n",
        "        return torch.sum(A @ V * G + (1-G) * x, dim=-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadWrite(nn.Module):\n",
        "    def __init__(self, in_dim, n_mem, mem_dim, key_dim, n_heads) -> None:\n",
        "        super(MultiHeadWrite, self).__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([WriteHead(in_dim, n_mem, mem_dim, key_dim//n_heads) for _ in range(n_heads)])\n",
        "        self.proj = nn.Linear(n_heads*mem_dim, mem_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, M: torch.Tensor) -> torch.Tensor:\n",
        "        M_w = [head(x, M) for head in self.heads]\n",
        "        return self.proj(torch.cat(M_w, dim=-1))\n",
        "\n",
        "\n",
        "class MultiHeadRead(nn.Module):\n",
        "    def __init__(self, in_dim, n_mem, mem_dim, key_dim, n_heads) -> None:\n",
        "        super(MultiHeadRead, self).__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([ReadHead(in_dim, n_mem, mem_dim, key_dim//n_heads) for _ in range(n_heads)])\n",
        "        self.proj = nn.Linear(n_heads*in_dim, in_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, M: torch.Tensor) -> torch.Tensor:\n",
        "        M_r = [head(x, M) for head in self.heads]\n",
        "        return self.proj(torch.cat(M_r, dim=-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Memory(nn.Module):\n",
        "    def __init__(self, in_dim, n_mem, mem_dim, key_dim, n_heads) -> None:\n",
        "        super(Memory, self).__init__()\n",
        "\n",
        "        self.register_buffer('M', torch.rand(1, n_mem, mem_dim))\n",
        "        self.write_block = MultiHeadWrite(in_dim, n_mem, mem_dim, key_dim, n_heads)\n",
        "        self.read_block = MultiHeadRead(in_dim, n_mem, mem_dim, key_dim, n_heads)\n",
        "\n",
        "    def reset_memory(self) -> None:\n",
        "        self.M = torch.rand(self.M.shape)\n",
        "        nn.init.xavier_normal_(self.M)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, op: str = None) -> torch.Tensor | None:\n",
        "        if x.ndim == 2:\n",
        "            x = x.unsqueeze(-2)\n",
        "        \n",
        "        if x.shape[0] > 1 and self.M.shape[0] != x.shape[0]:\n",
        "            self.M = self.M.expand(x.shape[0], *self.M.shape[1:])\n",
        "\n",
        "        if op == 'w' or op == None:\n",
        "            self.M = self.write_block(x, self.M)\n",
        "\n",
        "        if op == 'r' or op == None:\n",
        "            return self.read_block(x, self.M)\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IN_DIM = len_abc\n",
        "N_MEM = 32\n",
        "MEM_DIM = 32\n",
        "KEY_DIM = 16\n",
        "N_HEADS = 4\n",
        "\n",
        "T_N_BLOCKS = 4\n",
        "T_KEY_DIM = 16\n",
        "T_N_HEADS = 4\n",
        "T_H_DIM = 64\n",
        "\n",
        "memory = Memory(IN_DIM, N_MEM, MEM_DIM, KEY_DIM, N_HEADS)\n",
        "\n",
        "t_args = [MEM_DIM, T_KEY_DIM, T_N_HEADS, T_H_DIM]\n",
        "t_modules = [TransformerBlock(*t_args) for _ in range(T_N_BLOCKS)]\n",
        "transformer = nn.Sequential(*t_modules)\n",
        "\n",
        "classifier = nn.Sequential(\n",
        "    nn.Linear(len_abc, len_abc),\n",
        "    nn.Softmax(dim=1)\n",
        ")\n",
        "\n",
        "print(sum([p.numel() for p in memory.parameters()]))\n",
        "print(sum([p.numel() for p in transformer.parameters()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt = 'The quick brown fox jumps over the lazy dog.'\n",
        "encoded = encode(txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# UNBATCHED INPUT\n",
        "\n",
        "memory.reset_memory()\n",
        "for code in encoded:\n",
        "    seq = enc2seq([code])\n",
        "    memory(seq, op='w')\n",
        "    memory.M = transformer(memory.M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BATCHED INPUT\n",
        "\n",
        "memory.reset_memory()\n",
        "for code in encoded:\n",
        "    seq = enc2seq(code)\n",
        "    seq = seq.expand(32, *seq.shape)\n",
        "    memory(seq, op='w')\n",
        "    memory.M = transformer(memory.M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SEQUENCE GENERATION TEST\n",
        "\n",
        "n_tokens = 100\n",
        "block_size = 10\n",
        "result = []\n",
        "i, j = 0, 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    memory.reset_memory()\n",
        "\n",
        "    for _ in range(n_tokens):\n",
        "        if j == block_size:\n",
        "            memory.M = transformer(memory.M)\n",
        "            j = 0\n",
        "\n",
        "        m_r = memory(enc2seq([i]), op=None)\n",
        "        p = classifier(m_r)[0]\n",
        "        i = torch.multinomial(p, 1).item()\n",
        "        result.append(i)\n",
        "        \n",
        "        j += 1\n",
        "\n",
        "print(decode(result))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
