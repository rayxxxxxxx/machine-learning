{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path('..', '..', 'data', 'iris_csv.csv'))\n",
    "\n",
    "for c in df.columns[0:4]:\n",
    "    df[c] = (df[c]-df[c].mean())/df[c].std()\n",
    "\n",
    "df['synth1'] = df['petallength']*df['petalwidth']\n",
    "df['synth2'] = df['sepallength']*df['petallength']\n",
    "df['synth3'] = df['sepallength']*df['petalwidth']\n",
    "\n",
    "for name in df['class'].unique():\n",
    "    df[f'label-{name}'] = df['class'].map(lambda x: 1 if x == name else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "setosa_idxs = np.arange(0, 50)\n",
    "versicolor_idxs = np.arange(50, 100)\n",
    "virginica_idxs = np.arange(100, 150)\n",
    "\n",
    "p = np.random.permutation(np.arange(50))\n",
    "\n",
    "setosa_train_idxs = setosa_idxs[p[0:10]]\n",
    "setosa_test_idxs = setosa_idxs[p[10:]]\n",
    "\n",
    "versicolor_train_idxs = versicolor_idxs[p[0:10]]\n",
    "versicolor_test_idxs = versicolor_idxs[p[10:]]\n",
    "\n",
    "virginica_train_idxs = virginica_idxs[p[0:10]]\n",
    "virginica_test_idxs = virginica_idxs[p[10:]]\n",
    "\n",
    "feature_columns = ['sepallength', 'sepalwidth', 'petallength', 'petalwidth']\n",
    "label_columns = ['label-Iris-setosa', 'label-Iris-versicolor', 'label-Iris-virginica']\n",
    "\n",
    "xTrain = np.vstack([\n",
    "    df.iloc[setosa_train_idxs][feature_columns],\n",
    "    df.iloc[versicolor_train_idxs][feature_columns],\n",
    "    df.iloc[virginica_train_idxs][feature_columns]\n",
    "])\n",
    "\n",
    "yTrain = np.vstack([\n",
    "    df.iloc[setosa_train_idxs][label_columns],\n",
    "    df.iloc[versicolor_train_idxs][label_columns],\n",
    "    df.iloc[virginica_train_idxs][label_columns]\n",
    "])\n",
    "\n",
    "xTest = np.vstack([\n",
    "    df.iloc[setosa_test_idxs][feature_columns],\n",
    "    df.iloc[versicolor_test_idxs][feature_columns],\n",
    "    df.iloc[virginica_test_idxs][feature_columns]\n",
    "])\n",
    "\n",
    "yTest = np.vstack([\n",
    "    df.iloc[setosa_test_idxs][label_columns],\n",
    "    df.iloc[versicolor_test_idxs][label_columns],\n",
    "    df.iloc[virginica_test_idxs][label_columns]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(fastmath=True)\n",
    "def F(x: np.ndarray) -> np.ndarray:\n",
    "    # return x\n",
    "    # return np.maximum(np.zeros(x.shape), x)\n",
    "    # return np.clip(x,-1, 1)\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def dF(x: np.ndarray) -> np.ndarray:\n",
    "    # return np.ones(x.shape)\n",
    "    # return 1 * (x > 0)\n",
    "    # return np.array([0 if xi <= -1 or xi >= 1 else 1 for xi in x])\n",
    "    return 1-np.square(np.tanh(x))\n",
    "\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def dSigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    y = 1/(1+np.exp(-x))\n",
    "    return y*(1-y)\n",
    "\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    y = np.exp(x)\n",
    "    return y/np.sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(fastmath=True)\n",
    "def grads(xBatch: np.ndarray, w: np.ndarray, Bh:np.ndarray, b:np.ndarray) -> tuple[np.ndarray]:\n",
    "    dw = np.zeros(w.shape)\n",
    "    dBh = np.zeros(Bh.shape)\n",
    "    db = np.zeros(b.shape)\n",
    "    \n",
    "    for i in prange(xBatch.shape[0]):\n",
    "        u = xBatch[i] @ w + Bh\n",
    "        y = F(u) @ w.T + b\n",
    "\n",
    "        dLdy = 2/(w.shape[0]* w.shape[1]) * (y-xBatch[i])\n",
    "        \n",
    "        dw += np.outer(xBatch[i], dLdy @ w) * dF(u)\n",
    "        \n",
    "        dBh += (dLdy @ w) * dF(u)\n",
    "        db += dLdy\n",
    "    \n",
    "    return (dw, dBh, db)\n",
    "\n",
    "\n",
    "class RestrictedBoltzmannMachine:\n",
    "    def __init__(self, nIn: int, nHidden: int) -> None:\n",
    "        self.nIn = nIn\n",
    "        self.nHidden = nHidden\n",
    "        \n",
    "        self.w: np.ndarray = np.random.uniform(-1, 1, (nIn, nHidden))\n",
    "        \n",
    "        self.Bh: np.ndarray = np.zeros(nHidden)\n",
    "        self.b: np.ndarray = np.zeros(nIn)\n",
    "\n",
    "\n",
    "    def predict(self, x:np.ndarray) -> np.ndarray:\n",
    "        # return (x @ self.w + self.Bh) @ self.w.T + self.b\n",
    "        return F(x @ self.w + self.Bh) @ self.w.T + self.b\n",
    "\n",
    "\n",
    "    def train(self, xTrain: np.ndarray, lr, batch_size, max_iter) -> None:\n",
    "        n = xTrain.shape[0]\n",
    "\n",
    "        for k in range(max_iter):\n",
    "            idxs = np.random.choice(a=np.arange(n), size=batch_size, replace=False)\n",
    "            \n",
    "            dw, dBh, db = grads(xTrain[idxs], self.w, self.Bh, self.b)\n",
    "            \n",
    "            self.w -= lr*dw\n",
    "            self.Bh -= lr*dBh\n",
    "            self.b -= lr*db\n",
    "        \n",
    "    \n",
    "    def loss(self, x: np.ndarray) -> float:\n",
    "        xPred = np.array([self.predict(xi) for xi in x])\n",
    "        d = 1/self.nIn * np.linalg.norm(x-xPred, axis=1)\n",
    "        return 1/x.shape[0] * np.sum(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "untrained loss: 0.318504\n",
      "trained loss: 0.374042\n"
     ]
    }
   ],
   "source": [
    "nIn = 4\n",
    "nHidden = 2\n",
    "\n",
    "lr = 1e-2\n",
    "batch_size = 30\n",
    "max_iter = 5000\n",
    "\n",
    "model = RestrictedBoltzmannMachine(nIn, nHidden)\n",
    "\n",
    "print('untrained loss: {0:.6f}'.format(model.loss(xTest)))\n",
    "\n",
    "model.train(xTrain, lr, batch_size, max_iter)\n",
    "\n",
    "print('trained loss: {0:.6f}'.format(model.loss(xTest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26 [-0.41  2.64 -1.34 -1.31] [ 0.55  1.19  0.09 -0.19]\n",
      "1.1 [-1.02  0.34 -1.45 -1.31] [-0.1   0.29 -0.5   0.45]\n",
      "0.97 [-1.02  0.8  -1.22 -1.05] [-0.    0.59 -0.51  0.43]\n",
      "1.13 [-1.14  0.11 -1.28 -1.44] [-0.13 -0.02 -0.35  0.35]\n",
      "0.91 [-0.17  1.72 -1.17 -1.18] [ 0.47  1.1  -0.01 -0.09]\n",
      "1.08 [-1.02  0.8  -1.28 -1.31] [ 0.05  0.62 -0.43  0.36]\n",
      "1.11 [-0.05  2.18 -1.45 -1.31] [ 0.53  1.17  0.06 -0.16]\n",
      "1.06 [-1.26 -0.12 -1.34 -1.18] [-0.26 -0.28 -0.42  0.44]\n",
      "1.04 [-0.66  1.49 -1.28 -1.31] [ 0.31  0.96 -0.2   0.11]\n",
      "1.14 [-1.38  0.34 -1.22 -1.31] [-0.13  0.16 -0.47  0.44]\n",
      "1.3 [-0.17  3.1  -1.28 -1.05] [ 0.61  1.23  0.16 -0.25]\n",
      "1.1 [-1.26  0.11 -1.22 -1.31] [-0.17 -0.06 -0.4   0.4 ]\n",
      "1.29 [-0.78  2.41 -1.28 -1.44] [ 0.49  1.14  0.01 -0.11]\n",
      "1.04 [-0.54  1.95 -1.39 -1.05] [ 0.31  1.   -0.22  0.12]\n",
      "1.27 [-1.74  0.34 -1.39 -1.31] [-0.21  0.12 -0.58  0.55]\n",
      "0.95 [-0.54  0.8  -1.17 -1.31] [ 0.22  0.75 -0.23  0.15]\n",
      "0.89 [-1.02  1.03 -1.22 -0.78] [-0.03  0.66 -0.59  0.5 ]\n",
      "1.22 [-1.74 -0.36 -1.34 -1.31] [-0.36 -0.7  -0.34  0.41]\n",
      "1.35 [-1.86 -0.12 -1.51 -1.44] [-0.32 -0.41 -0.45  0.48]\n",
      "1.03 [-1.02 -0.12 -1.22 -1.31] [-0.18 -0.27 -0.3   0.33]\n",
      "0.99 [-0.54  1.95 -1.17 -1.05] [ 0.38  1.05 -0.12  0.02]\n",
      "1.02 [-0.9   1.03 -1.34 -1.18] [ 0.08  0.73 -0.45  0.36]\n",
      "1.07 [-1.02  1.03 -1.39 -1.18] [ 0.03  0.69 -0.51  0.42]\n",
      "1.08 [-1.02  0.57 -1.34 -1.31] [-0.02  0.47 -0.46  0.4 ]\n",
      "1.08 [-1.14 -0.12 -1.34 -1.31] [-0.22 -0.27 -0.36  0.39]\n",
      "1.16 [-1.26 -0.12 -1.34 -1.44] [-0.22 -0.32 -0.33  0.37]\n",
      "1.13 [-1.14  0.11 -1.28 -1.44] [-0.13 -0.02 -0.35  0.35]\n",
      "1.11 [-1.26  0.8  -1.05 -1.31] [ 0.06  0.57 -0.4   0.33]\n",
      "1.18 [-1.5   0.8  -1.34 -1.18] [-0.1   0.5  -0.62  0.54]\n",
      "0.87 [-0.9   0.57 -1.17 -0.92] [-0.05  0.47 -0.52  0.45]\n",
      "0.99 [-0.41  1.03 -1.39 -1.31] [ 0.23  0.85 -0.27  0.18]\n",
      "1.01 [-0.9   1.49 -1.28 -1.05] [ 0.15  0.85 -0.4   0.3 ]\n",
      "1.09 [-0.9   1.72 -1.28 -1.18] [ 0.25  0.93 -0.29  0.19]\n",
      "1.13 [-1.14  0.11 -1.28 -1.44] [-0.13 -0.02 -0.35  0.35]\n",
      "1.05 [-0.9   0.8  -1.28 -1.31] [ 0.08  0.65 -0.4   0.32]\n",
      "1.12 [-0.9   1.72 -1.22 -1.31] [ 0.31  0.98 -0.21  0.11]\n",
      "1.17 [-1.5   0.11 -1.28 -1.31] [-0.22 -0.1  -0.45  0.46]\n",
      "1.07 [-0.9   1.03 -1.34 -1.31] [ 0.12  0.75 -0.4   0.31]\n",
      "1.2 [-1.5   0.34 -1.34 -1.31] [-0.17  0.16 -0.53  0.5 ]\n",
      "1.01 [-0.9   1.72 -1.05 -1.05] [ 0.28  0.96 -0.25  0.15]\n",
      "0.43 [ 0.19 -0.82  0.76  0.53] [-0.09 -0.94  0.28 -0.13]\n",
      "0.37 [0.19 0.8  0.42 0.53] [ 0.33  0.81 -0.06 -0.01]\n",
      "0.58 [ 1.16 -0.59  0.59  0.26] [ 0.27 -0.41  0.59 -0.47]\n",
      "0.38 [-0.54 -0.12  0.42  0.39] [-0.12 -0.39 -0.11  0.17]\n",
      "0.53 [ 0.43 -1.97  0.42  0.39] [-0.33 -1.38  0.14  0.05]\n",
      "0.43 [-1.14 -1.51 -0.26 -0.26] [-0.47 -1.44 -0.07  0.26]\n",
      "0.11 [-0.29 -0.36 -0.09  0.13] [-0.18 -0.5  -0.15  0.22]\n",
      "0.15 [-0.17 -0.12  0.25  0.  ] [ 0.04 -0.25  0.08 -0.02]\n",
      "0.34 [-0.9  -1.28 -0.43 -0.13] [-0.46 -1.35 -0.11  0.28]\n",
      "0.21 [-0.17 -1.05 -0.15 -0.26] [-0.26 -1.13  0.1   0.06]\n",
      "0.45 [1.03 0.11 0.36 0.26] [ 0.41  0.45  0.29 -0.29]\n",
      "0.15 [-0.41 -1.51  0.02 -0.13] [-0.38 -1.37  0.04  0.14]\n",
      "0.1 [-0.05 -0.82  0.08  0.  ] [-0.19 -0.95  0.11  0.03]\n",
      "0.31 [-0.29 -0.12  0.42  0.39] [-0.06 -0.31 -0.05  0.11]\n",
      "0.1 [-0.05 -1.05  0.14  0.  ] [-0.23 -1.12  0.14  0.02]\n",
      "0.5 [0.07 0.34 0.59 0.79] [ 0.13  0.36 -0.14  0.11]\n",
      "0.41 [-1.02 -1.74 -0.26 -0.26] [-0.48 -1.48 -0.07  0.25]\n",
      "0.42 [ 0.91 -0.36  0.48  0.13] [ 0.29 -0.2   0.49 -0.4 ]\n",
      "0.28 [ 0.31 -0.36  0.53  0.26] [ 0.1  -0.41  0.29 -0.2 ]\n",
      "0.43 [ 0.91 -0.12  0.36  0.26] [ 0.3   0.12  0.31 -0.27]\n",
      "0.19 [-0.17 -0.59  0.42  0.13] [-0.1  -0.81  0.18 -0.05]\n",
      "0.1 [-0.05 -0.82  0.19 -0.26] [-0.08 -0.93  0.29 -0.14]\n",
      "0.15 [-0.41 -1.05  0.36  0.  ] [-0.26 -1.19  0.15  0.02]\n",
      "0.13 [-0.17 -0.59  0.19  0.13] [-0.16 -0.79  0.06  0.06]\n",
      "0.37 [0.67 0.34 0.42 0.39] [ 0.37  0.58  0.14 -0.17]\n",
      "0.41 [ 0.19 -1.97  0.14 -0.26] [-0.29 -1.35  0.19 -0.  ]\n",
      "0.46 [ 0.55 -1.74  0.36  0.13] [-0.21 -1.27  0.28 -0.09]\n",
      "0.32 [ 0.67 -0.36  0.31  0.13] [ 0.18 -0.25  0.33 -0.25]\n",
      "0.42 [0.55 0.57 0.53 0.53] [ 0.4   0.74  0.09 -0.14]\n",
      "0.2 [ 0.31 -0.59  0.53  0.  ] [ 0.11 -0.66  0.45 -0.32]\n",
      "0.51 [1.03 0.11 0.53 0.39] [ 0.41  0.42  0.32 -0.31]\n",
      "0.2 [ 0.31 -0.59  0.14  0.13] [-0.05 -0.63  0.16 -0.05]\n",
      "0.1 [-0.29 -1.28  0.08 -0.13] [-0.31 -1.27  0.1   0.08]\n",
      "0.27 [-0.78 -0.82  0.08  0.26] [-0.37 -1.11 -0.11  0.25]\n",
      "0.07 [-0.41 -1.28  0.14  0.13] [-0.37 -1.31  0.02  0.15]\n",
      "0.15 [-0.17 -0.36  0.25  0.13] [-0.07 -0.53  0.06  0.03]\n",
      "0.17 [-0.41 -1.74  0.14  0.13] [-0.43 -1.44 -0.    0.19]\n",
      "0.58 [1.4  0.34 0.53 0.26] [ 0.56  0.74  0.38 -0.4 ]\n",
      "0.22 [ 0.43 -0.36  0.31  0.13] [ 0.11 -0.34  0.26 -0.18]\n",
      "0.14 [-0.29 -0.82  0.25  0.13] [-0.24 -1.03  0.08  0.07]\n",
      "0.44 [ 0.55 -0.59  0.76  0.39] [ 0.12 -0.62  0.45 -0.32]\n",
      "1.38 [ 2.24 -0.12  1.33  1.44] [ 0.52  0.38  0.53 -0.5 ]\n",
      "0.53 [ 0.43 -0.59  0.59  0.79] [-0.08 -0.65  0.12 -0.01]\n",
      "0.42 [ 0.31 -1.05  1.04  0.26] [ 0.06 -0.98  0.57 -0.39]\n",
      "1.51 [ 2.24 -1.05  1.78  1.44] [ 0.36 -0.66  0.9  -0.73]\n",
      "1.03 [ 1.76 -0.36  1.44  0.79] [ 0.48 -0.16  0.79 -0.68]\n",
      "0.81 [-0.05 -0.59  0.76  1.57] [-0.32 -0.77 -0.22  0.31]\n",
      "0.95 [ 1.03 -0.12  0.82  1.44] [ 0.12  0.12 -0.    0.02]\n",
      "0.87 [0.43 0.8  0.93 1.44] [ 0.27  0.78 -0.16  0.09]\n",
      "0.84 [ 1.64 -0.12  1.16  0.53] [ 0.52  0.13  0.68 -0.61]\n",
      "0.88 [0.67 0.34 0.87 1.44] [ 0.18  0.5  -0.13  0.09]\n",
      "1.12 [ 1.88 -0.59  1.33  0.92] [ 0.42 -0.33  0.79 -0.66]\n",
      "0.89 [ 0.67 -0.59  1.04  1.31] [-0.04 -0.64  0.18 -0.07]\n",
      "0.68 [ 0.79 -0.12  0.99  0.79] [ 0.29 -0.02  0.38 -0.32]\n",
      "0.56 [-0.05 -0.82  0.76  0.92] [-0.25 -1.03  0.05  0.1 ]\n",
      "1.05 [1.28 0.34 1.1  1.44] [ 0.41  0.67  0.16 -0.2 ]\n",
      "1.05 [1.16 0.34 1.21 1.44] [ 0.41  0.62  0.19 -0.22]\n",
      "0.83 [ 1.03 -1.28  1.16  0.79] [ 0.07 -0.99  0.59 -0.4 ]\n",
      "0.57 [-0.17 -1.28  0.7   1.05] [-0.39 -1.32 -0.02  0.19]\n",
      "0.93 [1.64 0.34 1.27 0.79] [ 0.6   0.67  0.49 -0.5 ]\n",
      "1.3 [ 2.12 -0.12  1.61  1.18] [ 0.55  0.2   0.68 -0.62]\n",
      "1.34 [2.24 1.72 1.67 1.31] [ 0.75  1.31  0.36 -0.45]\n",
      "1.04 [1.03 0.11 1.04 1.57] [ 0.22  0.36  0.03 -0.04]\n",
      "0.53 [ 0.07 -0.12  0.76  0.79] [ 0.02 -0.23  0.04  0.02]\n",
      "0.56 [-0.05 -0.82  0.76  0.92] [-0.25 -1.03  0.05  0.1 ]\n",
      "0.92 [ 1.16 -0.12  0.99  1.18] [ 0.28  0.12  0.27 -0.24]\n",
      "0.66 [0.67 0.11 0.99 0.79] [ 0.33  0.23  0.3  -0.27]\n",
      "0.9 [1.03 0.57 1.1  1.18] [ 0.48  0.81  0.2  -0.24]\n",
      "0.47 [-1.14 -1.28  0.42  0.66] [-0.47 -1.41 -0.1   0.27]\n",
      "0.58 [ 0.55 -0.82  0.65  0.79] [-0.1  -0.85  0.2  -0.07]\n",
      "0.97 [0.55 0.8  1.04 1.57] [ 0.29  0.8  -0.13  0.05]\n",
      "0.58 [-0.29 -0.59  0.65  1.05] [-0.29 -0.86 -0.12  0.23]\n",
      "0.51 [ 0.19 -1.97  0.7   0.39] [-0.31 -1.37  0.17  0.02]\n",
      "1.26 [1.64 1.26 1.33 1.71] [ 0.64  1.19  0.23 -0.32]\n",
      "0.93 [1.28 0.11 0.93 1.18] [ 0.37  0.45  0.23 -0.23]\n",
      "0.66 [ 0.55 -1.28  0.7   0.92] [-0.24 -1.18  0.17  0.  ]\n",
      "0.66 [ 0.55 -0.36  1.04  0.79] [ 0.16 -0.4   0.39 -0.29]\n",
      "1.09 [0.55 0.57 1.27 1.71] [ 0.26  0.64 -0.08  0.03]\n",
      "0.74 [ 0.79 -0.12  0.82  1.05] [ 0.17  0.02  0.14 -0.1 ]\n",
      "1.1 [1.03 0.57 1.1  1.71] [ 0.34  0.77 -0.03 -0.04]\n"
     ]
    }
   ],
   "source": [
    "for x in xTest:\n",
    "    xPred = model.predict(x)\n",
    "    loss = 1/model.nHidden * np.linalg.norm(x-xPred).round(3)\n",
    "    print(round(loss,2), x.round(2), xPred.round(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
