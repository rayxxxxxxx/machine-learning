{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZeWuLqRyQRl"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "abc = ' ' + string.punctuation + string.digits + string.ascii_letters\n",
        "len_abc = len(abc)\n",
        "\n",
        "itoc = {i:c for i,c in enumerate(abc)}\n",
        "ctoi = {c:i for i,c in enumerate(abc)}\n",
        "\n",
        "def encode(s: str) -> list[int]:\n",
        "    return [ctoi[c] for c in s]\n",
        "\n",
        "def decode(l: list[int]) -> str:\n",
        "    return ''.join([itoc[i] for i in l])\n",
        "\n",
        "def enc2tnsr(l: list[int]) -> torch.Tensor:\n",
        "    return torch.tensor(l).long()\n",
        "\n",
        "def enc2seq(l: list[int]) -> torch.Tensor:\n",
        "    return F.one_hot(enc2tnsr(l), len_abc).float()\n",
        "\n",
        "def enct2seq(t: torch.Tensor) -> torch.Tensor:\n",
        "    return F.one_hot(t, len_abc).float()\n",
        "\n",
        "def str2seq(s: str) -> torch.Tensor:\n",
        "    encoded = torch.tensor(encode(s)).long()\n",
        "    return F.one_hot(encoded, len_abc).float()\n",
        "\n",
        "print(len_abc, abc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WriteHead(nn.Module):\n",
        "    def __init__(self, in_dim, n_mem, mem_dim, key_dim) -> None:\n",
        "        super(WriteHead, self).__init__()\n",
        "\n",
        "        self.Wq = nn.Linear(in_dim, key_dim)\n",
        "        self.Wk = nn.Linear(mem_dim, key_dim)\n",
        "        self.Wv = nn.Linear(in_dim, mem_dim)\n",
        "        self.Wg = nn.Parameter(torch.rand(mem_dim, n_mem))\n",
        "\n",
        "    def forward(self, x: torch.Tensor, M: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.unsqueeze(-2)\n",
        "        Q, K, V = self.Wq(x), self.Wk(M), self.Wv(x)\n",
        "        a = F.softmax(K @ Q.transpose(-1,-2), dim=-2)\n",
        "        g = F.sigmoid(V @ self.Wg @ M)\n",
        "        return a @ V * g\n",
        "\n",
        "\n",
        "class ReadHead(nn.Module):\n",
        "    def __init__(self, in_dim, n_mem, mem_dim, key_dim) -> None:\n",
        "        super(ReadHead, self).__init__()\n",
        "\n",
        "        self.Wq = nn.Linear(in_dim, key_dim)\n",
        "        self.Wk = nn.Linear(mem_dim, key_dim)\n",
        "        self.Wv = nn.Linear(mem_dim, in_dim)\n",
        "        self.Wg = nn.Parameter(torch.rand(in_dim, n_mem))\n",
        "\n",
        "    def forward(self, x: torch.Tensor, M: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.unsqueeze(-2)\n",
        "        Q, K, V = self.Wq(x), self.Wk(M), self.Wv(M)\n",
        "        a = F.softmax(Q @ K.transpose(-1,-2), dim=-1)\n",
        "        g = F.sigmoid(x @ self.Wg @ V)\n",
        "        return torch.sum(a @ V * g, dim=-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WriteHeadBlock(nn.Module):\n",
        "    def __init__(self, in_dim, n_mem, mem_dim, key_dim, n_heads) -> None:\n",
        "        super(WriteHeadBlock, self).__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([WriteHead(in_dim, n_mem, mem_dim, key_dim//n_heads) for _ in range(n_heads)])\n",
        "        self.W_o = nn.Linear(n_heads*mem_dim, mem_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, M: torch.Tensor) -> torch.Tensor:\n",
        "        M_w = [head(x, M) for head in self.heads]\n",
        "        return self.W_o(torch.cat(M_w, dim=-1))\n",
        "\n",
        "\n",
        "class ReadHeadBlock(nn.Module):\n",
        "    def __init__(self, in_dim, n_mem, mem_dim, key_dim, n_heads) -> None:\n",
        "        super(ReadHeadBlock, self).__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([ReadHead(in_dim, n_mem, mem_dim, key_dim//n_heads) for _ in range(n_heads)])\n",
        "        self.W_o = nn.Linear(n_heads*in_dim, in_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, M: torch.Tensor) -> torch.Tensor:\n",
        "        M_r = [head(x, M) for head in self.heads]\n",
        "        return self.W_o(torch.cat(M_r, dim=-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Memory(nn.Module):\n",
        "    def __init__(self, in_dim, n_mem, mem_dim, key_dim, n_heads) -> None:\n",
        "        super(Memory, self).__init__()\n",
        "\n",
        "        self.register_buffer('M', (-1+2*torch.rand(1, n_mem, mem_dim)))\n",
        "        self.write_block = WriteHeadBlock(in_dim, n_mem, mem_dim, key_dim, n_heads)\n",
        "        self.read_block = ReadHeadBlock(in_dim, n_mem, mem_dim, key_dim, n_heads)\n",
        "\n",
        "    def reset_memory(self) -> None:\n",
        "        self.M = -1+2*torch.rand(self.M.shape)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if x.shape[0] > 1 or self.M.shape[0] != x.shape[0]:\n",
        "            self.M = self.M.expand(x.shape[0], *self.M.shape[1:])\n",
        "\n",
        "        self.M = self.M + self.write_block(x, self.M)\n",
        "        return self.read_block(x, self.M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, emb_dim, n_heads, h_dim) -> None:\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.mha = nn.MultiheadAttention(emb_dim, n_heads, batch_first=False)\n",
        "        self.mlp = nn.Sequential(nn.Linear(emb_dim, h_dim), nn.ReLU(), nn.Linear(h_dim, emb_dim))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        attn_out, _ = self.mha(x, x, x)\n",
        "        return self.mlp(x + attn_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJWAKffU5Ssg"
      },
      "outputs": [],
      "source": [
        "class RNNNet(nn.Module):\n",
        "    def __init__(self, rnns_args: tuple, mem_args: tuple) -> None:\n",
        "        super(RNNNet, self).__init__()\n",
        "\n",
        "        n_rnn, in_dim, h_dim, n_heads, th_dim = rnns_args\n",
        "        n_mem, mem_dim, mem_key_dim, n_mem_heads = mem_args\n",
        "\n",
        "        self.n_rnn = n_rnn\n",
        "        self.h_dim = h_dim\n",
        "\n",
        "        self.register_buffer('states', torch.zeros(n_rnn, 1, h_dim))\n",
        "        self.rnns = nn.ModuleList([nn.RNN(in_dim, h_dim, batch_first=True) for _ in range(n_rnn)])\n",
        "        self.tblock = TransformerBlock(h_dim, n_heads, th_dim)\n",
        "        self.mems = nn.ModuleList([Memory(h_dim, n_mem, mem_dim, mem_key_dim, n_mem_heads) for _ in range(n_mem)])\n",
        "\n",
        "    def reset_states(self) -> None:\n",
        "        self.states = torch.zeros(self.n_rnn, 1, self.h_dim)\n",
        "\n",
        "    def reset_memory(self) -> None:\n",
        "        for mem in self.mems:\n",
        "            mem.reset_memory()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        #TODO: handle batched input\n",
        "        \n",
        "        c = torch.cat([rnn(x, s)[1] for rnn, s in zip(self.rnns, self.states)], dim=0)\n",
        "        self.states = (c + self.tblock(c)).unsqueeze(-2)\n",
        "        \n",
        "        for i in range(len(self.rnns)):\n",
        "            self.states[i] = self.mems[i](self.states[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLjHuoU3EWLv",
        "outputId": "6874537d-38dc-4ab3-fba2-ca460a953c66"
      },
      "outputs": [],
      "source": [
        "IN_DIM = 10\n",
        "N_RNN = 3\n",
        "H_DIM = 100\n",
        "N_HEADS = 4\n",
        "TH_DIM = 100\n",
        "\n",
        "N_MEM = 3\n",
        "MEM_DIM = 100\n",
        "MEM_KEY_DIM = 10\n",
        "N_MEM_HEADS = 2\n",
        "\n",
        "rnn_args = (N_RNN, IN_DIM, H_DIM, N_HEADS, TH_DIM)\n",
        "mem_args = (N_MEM, MEM_DIM, MEM_KEY_DIM, N_MEM_HEADS)\n",
        "\n",
        "model = RNNNet(rnn_args, mem_args)\n",
        "optim = torch.optim.SGD(model.parameters())\n",
        "\n",
        "seq = torch.ones(10, IN_DIM); print(seq.shape)\n",
        "chunks = torch.split(seq, 8); print(len(chunks), chunks[0].shape)\n",
        "\n",
        "batch = torch.stack([seq, seq]); print(batch.shape)\n",
        "batch_chunks = torch.split(batch, 8, dim=1); print(len(batch_chunks), batch_chunks[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqovIRgpE6o0",
        "outputId": "b8feb8fe-68ce-4b62-d849-d3cfe6ab8013"
      },
      "outputs": [],
      "source": [
        "for chnk in chunks:\n",
        "    model.states = torch.zeros(N_RNN, 1, H_DIM)\n",
        "    model.reset_memory()\n",
        "    \n",
        "    optim.zero_grad()\n",
        "    model(chnk)\n",
        "    model.states.sum().backward()\n",
        "    optim.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for bchnk in batch_chunks:\n",
        "    model.states = torch.zeros(N_RNN, 1, 2, H_DIM)\n",
        "    model.reset_memory()\n",
        "    \n",
        "    optim.zero_grad()\n",
        "    model(bchnk)\n",
        "    model.states.sum().backward()\n",
        "    optim.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "IN_DIM = len_abc\n",
        "N_RNN = 4\n",
        "H_DIM = 64\n",
        "N_HEADS = 8\n",
        "TH_DIM = 64\n",
        "\n",
        "N_MEM = 32\n",
        "MEM_DIM = 32\n",
        "MEM_KEY_DIM = 16\n",
        "N_MEM_HEADS = 4\n",
        "\n",
        "rnns_args = (N_RNN, IN_DIM, H_DIM, N_HEADS, TH_DIM)\n",
        "mem_args = (N_MEM, MEM_DIM, MEM_KEY_DIM, N_MEM_HEADS)\n",
        "model = RNNNet(rnns_args, mem_args)\n",
        "\n",
        "txt = 'The quick brown fox jumps over the lazy dog.'\n",
        "chunks = [encode(chnk) for chnk in txt.split(' ')]\n",
        "\n",
        "model.reset_states()\n",
        "model.reset_memory()\n",
        "for chnk in chunks:\n",
        "    seq = enc2seq(chnk)\n",
        "    model.forward(seq)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".pytorch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
