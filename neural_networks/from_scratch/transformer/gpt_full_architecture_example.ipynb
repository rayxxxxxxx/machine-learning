{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 11459,
     "status": "ok",
     "timestamp": 1743703639899,
     "user": {
      "displayName": "rayxxx",
      "userId": "02334739596992171567"
     },
     "user_tz": -180
    },
    "id": "wI-CMmP6WtGr"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc=' !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
      "len_abc=95\n"
     ]
    }
   ],
   "source": [
    "abc = ' ' + string.punctuation + string.digits + string.ascii_letters\n",
    "len_abc = len(abc)\n",
    "print(f'{abc=}\\n{len_abc=}')\n",
    "\n",
    "itoc = {i:c for i,c in enumerate(abc)}\n",
    "ctoi = {c:i for i,c in enumerate(abc)}\n",
    "\n",
    "def encode(s: str) -> list[int]:\n",
    "    return [ctoi[c] for c in s]\n",
    "\n",
    "def decode(l: list[int]) -> str:\n",
    "    return ''.join([itoc[i] for i in l])\n",
    "\n",
    "def enc2tnsr(l: list[int]) -> torch.Tensor:\n",
    "    return torch.tensor(l).long()\n",
    "\n",
    "def enc2seq(l: list[int]) -> torch.Tensor:\n",
    "    return F.one_hot(enc2tnsr(l), len_abc).float()\n",
    "\n",
    "def enct2seq(t: torch.Tensor) -> torch.Tensor:\n",
    "    return F.one_hot(t, len_abc).float()\n",
    "\n",
    "def str2seq(s: str) -> torch.Tensor:\n",
    "    encoded = torch.tensor(encode(s)).long()\n",
    "    return F.one_hot(encoded, len_abc).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1743703639950,
     "user": {
      "displayName": "rayxxx",
      "userId": "02334739596992171567"
     },
     "user_tz": -180
    },
    "id": "bMBnE4u7lRUq"
   },
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, in_dim, key_dim) -> None:\n",
    "        super(AttentionHead, self).__init__()\n",
    "\n",
    "        self.scaler = key_dim**-0.5\n",
    "\n",
    "        self.q = nn.Linear(in_dim, key_dim, bias=False)\n",
    "        self.k = nn.Linear(in_dim, key_dim, bias=False)\n",
    "        self.v = nn.Linear(in_dim, in_dim, bias=False)\n",
    "\n",
    "    def forward(self, Xq: torch.Tensor, Xk: torch.Tensor, Xv: torch.Tensor, masked: bool = False) -> torch.Tensor:\n",
    "        Q, K, V = self.q(Xq), self.k(Xk), self.v(Xv)\n",
    "        A = (Q @ K.transpose(1,2)) * self.scaler\n",
    "        if masked:\n",
    "            mask = torch.tril(torch.ones(len(Xq), len(Xq)))==0\n",
    "            A = A.masked_fill(mask, -torch.inf)\n",
    "        A = F.softmax(A, dim=2)\n",
    "        return A @ V\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, in_dim, key_dim) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([AttentionHead(in_dim, key_dim//n_heads) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_heads*in_dim, in_dim)\n",
    "\n",
    "    def forward(self, Xq: torch.Tensor, Xk: torch.Tensor, Xv: torch.Tensor, masked: bool = False) -> torch.Tensor:\n",
    "        V = torch.cat([h(Xq, Xk, Xv, masked) for h in self.heads], dim=2)\n",
    "        return self.proj(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, n_heads, in_dim, key_dim, h_dim) -> None:\n",
    "        super(EncoderBlock, self).__init__()\n",
    "    \n",
    "        self.attention = MultiHeadAttention(n_heads, in_dim, key_dim)\n",
    "        self.mlp = nn.Sequential(nn.Linear(in_dim, h_dim), nn.ReLU(), nn.Linear(h_dim, in_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.attention(x, x, x)\n",
    "        return x + self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1743703640015,
     "user": {
      "displayName": "rayxxx",
      "userId": "02334739596992171567"
     },
     "user_tz": -180
    },
    "id": "28hRvp05pd4p"
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, n_heads, in_dim, key_dim, h_dim) -> None:\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(n_heads, in_dim, key_dim)\n",
    "        self.cross_attn = MultiHeadAttention(n_heads, in_dim, key_dim)\n",
    "        self.mlp = nn.Sequential(nn.Linear(in_dim, h_dim), nn.ReLU(), nn.Linear(h_dim, in_dim))\n",
    "\n",
    "    def forward(self, Xq: torch.Tensor, Xkv: torch.Tensor) -> torch.Tensor:\n",
    "        Xq = Xq + self.self_attn(Xq, Xq, Xq, masked=True)\n",
    "        Xq = Xq + self.cross_attn(Xq, Xkv, Xkv)\n",
    "        return Xq + self.mlp(Xq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIM = len_abc\n",
    "KEY_DIM = 16\n",
    "N_HEADS = 4\n",
    "H_DIM = len_abc*2\n",
    "N_BLOCKS = 4\n",
    "OUT_DIM = len_abc\n",
    "\n",
    "args = [N_HEADS, IN_DIM, KEY_DIM, H_DIM]\n",
    "\n",
    "encoder_modules = [EncoderBlock(*args) for _ in range(N_BLOCKS)]\n",
    "encoder = EncoderBlock(*args)\n",
    "\n",
    "decoder_modules = [DecoderBlock(*args) for _ in range(N_BLOCKS)]\n",
    "decoder = DecoderBlock(*args)\n",
    "\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(IN_DIM, OUT_DIM),\n",
    "    nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A;xOJqSf3Z9/sB|#E@A67!cCCze}gm)]tAq gmlAx#m -wmwqe9]_fho1i{^p!}A1V9@`+A^no-?%HwGSm[M$uD^;Md]Ni; B|?{\n"
     ]
    }
   ],
   "source": [
    "prompt = 'The quick brown fox jumps over the lazy dog'\n",
    "enc = encode(prompt)\n",
    "seq = enc2seq(enc).unsqueeze(0)\n",
    "\n",
    "history = encoder.forward(seq)\n",
    "\n",
    "out = [0]\n",
    "for _ in range(100):\n",
    "    seq = enc2seq(out).unsqueeze(0)\n",
    "    dec_out = decoder.forward(seq, history)\n",
    "    p = classifier(dec_out[0])[0]\n",
    "    i = torch.multinomial(p,1).item()\n",
    "\n",
    "    t = enc2seq([i]).unsqueeze(0)\n",
    "    history = torch.cat((history, t), dim=1)\n",
    "    out.append(i)\n",
    "\n",
    "print(decode(out))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPGvDVBzl6uSnc/wFqeR5mR",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".pytorch (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
