{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3762df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f43fe14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n"
     ]
    }
   ],
   "source": [
    "abc = ' ' + string.punctuation + string.digits + string.ascii_letters\n",
    "len_abc = len(abc)\n",
    "\n",
    "itoc = {i:c for i,c in enumerate(abc)}\n",
    "ctoi = {c:i for i,c in enumerate(abc)}\n",
    "\n",
    "print(len_abc, abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5342f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(s: str) -> list[int]:\n",
    "    return [ctoi[c] for c in s]\n",
    "\n",
    "def decode(l: list[int]) -> str:\n",
    "    return ''.join([itoc[i] for i in l])\n",
    "\n",
    "def enc2tnsr(l: list[int]) -> torch.Tensor:\n",
    "    return torch.tensor(l).long()\n",
    "\n",
    "def enc2seq(l: list[int]) -> torch.Tensor:\n",
    "    return F.one_hot(enc2tnsr(l), len_abc).float()\n",
    "\n",
    "def enct2seq(t: torch.Tensor) -> torch.Tensor:\n",
    "    return F.one_hot(t, len_abc).float()\n",
    "\n",
    "def str2seq(s: str) -> torch.Tensor:\n",
    "    encoded = torch.tensor(encode(s)).long()\n",
    "    return F.one_hot(encoded, len_abc).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "aa39bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveRNN(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, window_size) -> None:\n",
    "        super(AttentiveRNN, self).__init__()\n",
    "\n",
    "        self.idx = 0\n",
    "        self.window_size = window_size\n",
    "        self.register_buffer('S', torch.zeros(window_size, h_dim))\n",
    "\n",
    "        self.rnn = nn.RNN(in_dim, h_dim, batch_first=True)\n",
    "        self.W = nn.Parameter(torch.rand(h_dim, h_dim))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        _, h = self.rnn(x)\n",
    "        a = F.softmax(self.S @ h.T, dim=-2)\n",
    "        c = torch.sum(a * self.S, dim=-2)\n",
    "        a = F.sigmoid(h @ self.W @ c.T)\n",
    "        g = F.sigmoid(h * c)\n",
    "        self.S[self.idx] = a * (1-g) * h + (1-a) * g * c\n",
    "        self.idx = (self.idx + 1) % self.window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "05c3209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "arnn = AttentiveRNN(10, 5, 3)\n",
    "X = [torch.rand(4, 10) for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "34f4fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in X:\n",
    "    arnn(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
