{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from numba import njit, prange\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(x: np.ndarray) -> float:\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def F_jit(x: np.ndarray) -> float:\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def dF_jit(x: np.ndarray) -> float:\n",
    "    y = 1/(1+np.exp(-x))\n",
    "    return y*(1-y)\n",
    "\n",
    "\n",
    "def Fh(x: np.ndarray) -> float:\n",
    "    # return np.clip(x,-1, 1)\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def Fh_jit(x: np.ndarray) -> np.ndarray:\n",
    "    # return np.clip(x,-1, 1)\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def dFh_jit(x: np.ndarray) -> np.ndarray:\n",
    "    # return np.array([0 if xi <= -1 or xi >= 1 else 1 for xi in x])\n",
    "    return 1-np.square(np.tanh(x))\n",
    "\n",
    "\n",
    "@njit\n",
    "def grads(xBatch: np.ndarray, yBatch: np.ndarray, Wy: np.ndarray, Wh: np.ndarray, Bu:np.ndarray, Bh: np.ndarray, b:np.ndarray) -> tuple[np.ndarray]:\n",
    "    n = xBatch.shape[0]\n",
    "    nIn = Wy.shape[0]\n",
    "\n",
    "    dWy = np.zeros(Wy.shape)\n",
    "    dWh = np.zeros(Wh.shape)\n",
    "    dBu = np.zeros(Bu.shape)\n",
    "    dBh = np.zeros(Bh.shape)\n",
    "    db = np.zeros(b.shape)\n",
    "    \n",
    "    for i in prange(n):\n",
    "        Uh = xBatch[i] @ Wh + Bu\n",
    "        Yh = Fh_jit(Uh) @ Wh.T + Bh\n",
    "        u = Yh @ Wy + b\n",
    "        y = F_jit(u)\n",
    "\n",
    "        dLdu = 2/nIn * (y-yBatch[i])*dF_jit(u)\n",
    "        \n",
    "        dWh += dLdu @ Wy.T @ Wh * dFh_jit(Uh) * np.atleast_2d(xBatch[i]).T\n",
    "        dWy += dLdu * np.atleast_2d(Yh).T\n",
    "        dBu += dLdu @ Wy.T @ Wh * dFh_jit(Uh)\n",
    "        dBh += dLdu @ Wy.T\n",
    "        db += dLdu\n",
    "    \n",
    "    return (dWh, dWy, dBu, dBh, db)\n",
    "\n",
    "\n",
    "class RBMPerceptron:\n",
    "    def __init__(self, nH:int, nIn: int, nOut: int) -> None:\n",
    "        self.nH = nH\n",
    "        self.nIn = nIn\n",
    "        self.nOut = nOut\n",
    "        \n",
    "        self.Wh: np.ndarray = np.random.uniform(-1, 1, (nIn, nH))\n",
    "        self.Wy: np.ndarray = np.random.uniform(-1, 1, (nIn, nOut))\n",
    "\n",
    "        self.Bu: np.ndarray = np.zeros(nH)\n",
    "        self.Bh: np.ndarray = np.zeros(nIn)\n",
    "        self.b: np.ndarray = np.zeros(nOut)\n",
    "\n",
    "\n",
    "    def predict(self, x:np.ndarray) -> np.ndarray:\n",
    "        Uh = x @ self.Wh + self.Bu\n",
    "        Yh = Fh(Uh) @ self.Wh.T + self.Bh\n",
    "        return F(Yh @ self.Wy + self.b)\n",
    "\n",
    "\n",
    "    def train(self, xTrain: np.ndarray, yTrain: np.ndarray, lr, batch_size, max_iter) -> None:\n",
    "        n = xTrain.shape[0]\n",
    "\n",
    "        for k in range(max_iter):\n",
    "            idxs = np.random.choice(a=np.arange(n), size=batch_size, replace=False)\n",
    "            \n",
    "            dWh, dWy, dBu, dBh, db = grads(xTrain[idxs], yTrain[idxs], self.Wy, self.Wh, self.Bu, self.Bh, self.b)\n",
    "            \n",
    "            self.Wy -= lr*dWy\n",
    "            self.Wh -= lr*dWh\n",
    "            self.Bu -= lr*dBu\n",
    "            self.Bh -= lr*dBh\n",
    "            self.b -= lr*db\n",
    "        \n",
    "    \n",
    "    def loss(self, x: np.ndarray, y: np.ndarray) -> float:\n",
    "        n = x.shape[0]\n",
    "        \n",
    "        d = np.array([1/self.nIn*np.sum(np.square(self.predict(xi)-yi)) for xi,yi in zip(x,y)])\n",
    "        \n",
    "        return 1/n*np.sum(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path('..', '..', 'data', 'iris_csv.csv'))\n",
    "\n",
    "for c in df.columns[0:4]:\n",
    "    df[c] = (df[c]-df[c].mean())/df[c].std()\n",
    "\n",
    "df['synth1'] = df['petallength']*df['petalwidth']\n",
    "df['synth2'] = df['sepallength']*df['petallength']\n",
    "df['synth3'] = df['sepallength']*df['petalwidth']\n",
    "\n",
    "for name in df['class'].unique():\n",
    "    df[f'{name}_label'] = df['class'].map(lambda x: 1 if x == name else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frac = 0.8\n",
    "\n",
    "np.random.seed(0)\n",
    "p = np.random.permutation(df.index.size)\n",
    "\n",
    "test_size = int(p.size*test_frac)\n",
    "train_size = int(p.size*(1-test_frac))\n",
    "\n",
    "idx_test = p[0 : test_size]\n",
    "idx_train = p[test_size: p.size]\n",
    "\n",
    "features_columns = ['sepallength', 'sepalwidth', 'petallength', 'petalwidth']\n",
    "label_columns = ['Iris-setosa_label', 'Iris-versicolor_label', 'Iris-virginica_label']\n",
    "\n",
    "xTest = np.array(df.iloc[idx_test][features_columns])\n",
    "yTest = np.array(df.iloc[idx_test][label_columns])\n",
    "\n",
    "xTrain = np.array(df.iloc[idx_train][features_columns])\n",
    "yTrain = np.array(df.iloc[idx_train][label_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "untrained loss: 0.320013\n",
      "trained loss: 0.022419\n"
     ]
    }
   ],
   "source": [
    "nH = 4\n",
    "nIn = 4\n",
    "nOut = 3\n",
    "\n",
    "lr = 1e-2\n",
    "batch_size = 15\n",
    "max_iter = 5000\n",
    "\n",
    "model = RBMPerceptron(nH, nIn, nOut)\n",
    "\n",
    "print('untrained loss: {0:.6f}'.format(model.loss(xTest, yTest)))\n",
    "\n",
    "model.train(xTrain, yTrain, lr, batch_size, max_iter)\n",
    "\n",
    "print('trained loss: {0:.6f}'.format(model.loss(xTest, yTest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.07 0.99] [0 0 1]\n",
      "[0.06 0.95 0.01] [0 1 0]\n",
      "[0.99 0.04 0.  ] [1 0 0]\n",
      "[0.   0.17 0.96] [0 0 1]\n",
      "[0.99 0.1  0.  ] [1 0 0]\n",
      "[0.   0.03 0.99] [0 0 1]\n",
      "[0.99 0.09 0.  ] [1 0 0]\n",
      "[0.09 0.86 0.03] [0 1 0]\n",
      "[0.06 0.89 0.03] [0 1 0]\n",
      "[0.09 0.93 0.01] [0 1 0]\n",
      "[0.   0.54 0.63] [0 0 1]\n",
      "[0.12 0.85 0.02] [0 1 0]\n",
      "[0.06 0.91 0.02] [0 1 0]\n",
      "[0.06 0.87 0.04] [0 1 0]\n",
      "[0.05 0.87 0.04] [0 1 0]\n",
      "[0.98 0.13 0.  ] [1 0 0]\n",
      "[0.06 0.86 0.04] [0 1 0]\n",
      "[0.04 0.91 0.03] [0 1 0]\n",
      "[0.98 0.13 0.  ] [1 0 0]\n",
      "[0.99 0.05 0.  ] [1 0 0]\n",
      "[0.   0.26 0.89] [0 0 1]\n",
      "[0.06 0.84 0.04] [0 1 0]\n",
      "[0.99 0.11 0.  ] [1 0 0]\n",
      "[0.98 0.14 0.  ] [1 0 0]\n",
      "[0.02 0.61 0.33] [0 0 1]\n",
      "[0.99 0.09 0.  ] [1 0 0]\n",
      "[0.99 0.06 0.  ] [1 0 0]\n",
      "[0.09 0.92 0.01] [0 1 0]\n",
      "[0.07 0.95 0.01] [0 1 0]\n",
      "[0.99 0.09 0.  ] [1 0 0]\n",
      "[0.01 0.36 0.68] [0 0 1]\n",
      "[0.05 0.82 0.05] [0 1 0]\n",
      "[0.99 0.09 0.  ] [1 0 0]\n",
      "[0.02 0.58 0.3 ] [0 0 1]\n",
      "[0.   0.08 0.99] [0 0 1]\n",
      "[0.06 0.91 0.02] [0 1 0]\n",
      "[0.99 0.06 0.  ] [1 0 0]\n",
      "[0.01 0.61 0.44] [0 1 0]\n",
      "[0.09 0.88 0.02] [0 1 0]\n",
      "[0.07 0.94 0.01] [0 1 0]\n",
      "[0.   0.16 0.93] [0 0 1]\n",
      "[0.98 0.11 0.  ] [1 0 0]\n",
      "[0.01 0.17 0.86] [0 0 1]\n",
      "[0.99 0.07 0.  ] [1 0 0]\n",
      "[0.99 0.08 0.  ] [1 0 0]\n",
      "[0.05 0.95 0.01] [0 1 0]\n",
      "[0.01 0.36 0.72] [0 0 1]\n",
      "[0.   0.04 0.99] [0 0 1]\n",
      "[0.01 0.6  0.59] [0 0 1]\n",
      "[0.   0.2  0.96] [0 0 1]\n",
      "[0.07 0.94 0.01] [0 1 0]\n",
      "[0.   0.06 0.99] [0 0 1]\n",
      "[0.1  0.79 0.03] [0 1 0]\n",
      "[0.06 0.95 0.01] [0 1 0]\n",
      "[0.01 0.56 0.46] [0 0 1]\n",
      "[0.02 0.76 0.16] [0 0 1]\n",
      "[0.01 0.54 0.62] [0 0 1]\n",
      "[0.   0.36 0.84] [0 0 1]\n",
      "[0.12 0.88 0.02] [0 1 0]\n",
      "[0.01 0.3  0.76] [0 0 1]\n",
      "[0.17 0.75 0.02] [0 1 0]\n",
      "[0.98 0.12 0.  ] [1 0 0]\n",
      "[0.   0.28 0.9 ] [0 0 1]\n",
      "[0.06 0.92 0.02] [0 1 0]\n",
      "[0.13 0.91 0.01] [0 1 0]\n",
      "[0.05 0.93 0.02] [0 1 0]\n",
      "[0.07 0.88 0.03] [0 1 0]\n",
      "[0.01 0.28 0.76] [0 0 1]\n",
      "[0.99 0.08 0.  ] [1 0 0]\n",
      "[0.98 0.13 0.  ] [1 0 0]\n",
      "[0.   0.29 0.88] [0 0 1]\n",
      "[0.11 0.91 0.01] [0 1 0]\n",
      "[0.99 0.1  0.  ] [1 0 0]\n",
      "[1.   0.03 0.  ] [1 0 0]\n",
      "[0.08 0.85 0.03] [0 1 0]\n",
      "[0.98 0.12 0.  ] [1 0 0]\n",
      "[0.01 0.44 0.57] [0 0 1]\n",
      "[0.1  0.95 0.01] [0 1 0]\n",
      "[0.99 0.1  0.  ] [1 0 0]\n",
      "[0.02 0.83 0.14] [0 1 0]\n",
      "[0.   0.03 0.99] [0 0 1]\n",
      "[0.08 0.91 0.02] [0 1 0]\n",
      "[0.98 0.13 0.  ] [1 0 0]\n",
      "[0.   0.11 0.97] [0 0 1]\n",
      "[0.   0.07 0.97] [0 0 1]\n",
      "[0.   0.07 0.99] [0 0 1]\n",
      "[0.   0.11 0.98] [0 0 1]\n",
      "[0.99 0.07 0.  ] [1 0 0]\n",
      "[0.99 0.1  0.  ] [1 0 0]\n",
      "[0.02 0.37 0.52] [0 0 1]\n",
      "[0.01 0.12 0.93] [0 0 1]\n",
      "[0.8  0.57 0.  ] [1 0 0]\n",
      "[0.01 0.07 0.96] [0 0 1]\n",
      "[0.98 0.13 0.  ] [1 0 0]\n",
      "[0.   0.2  0.96] [0 0 1]\n",
      "[0.01 0.21 0.84] [0 0 1]\n",
      "[0.98 0.12 0.  ] [1 0 0]\n",
      "[0.99 0.09 0.  ] [1 0 0]\n",
      "[0.02 0.56 0.4 ] [0 0 1]\n",
      "[0.99 0.09 0.  ] [1 0 0]\n",
      "[0.98 0.13 0.  ] [1 0 0]\n",
      "[0.99 0.05 0.  ] [1 0 0]\n",
      "[0.04 0.92 0.03] [0 1 0]\n",
      "[0.   0.05 0.99] [0 0 1]\n",
      "[0.   0.06 0.99] [0 0 1]\n",
      "[0.99 0.09 0.  ] [1 0 0]\n",
      "[0.98 0.13 0.  ] [1 0 0]\n",
      "[0.99 0.1  0.  ] [1 0 0]\n",
      "[0.05 0.9  0.03] [0 1 0]\n",
      "[0.1  0.9  0.02] [0 1 0]\n",
      "[0.98 0.11 0.  ] [1 0 0]\n",
      "[0.99 0.1  0.  ] [1 0 0]\n",
      "[0.09 0.92 0.01] [0 1 0]\n",
      "[0.99 0.09 0.  ] [1 0 0]\n",
      "[0.   0.05 1.  ] [0 0 1]\n",
      "[0.08 0.95 0.01] [0 1 0]\n",
      "[0.02 0.16 0.74] [0 0 1]\n",
      "[0.12 0.89 0.01] [0 1 0]\n",
      "[0.99 0.06 0.  ] [1 0 0]\n",
      "[0.02 0.61 0.26] [0 0 1]\n"
     ]
    }
   ],
   "source": [
    "for x,y in zip(xTest, yTest):\n",
    "    print(model.predict(x).round(2), y.round(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
